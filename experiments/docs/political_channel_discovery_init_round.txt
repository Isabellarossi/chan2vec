
# NOTE: THIS IS NOT A SCRIPT, MANY COMMANDS INVOLVE CREATING AWS INSTANCES

# Make all the necessary directories
mkdir -p data/pol_chan_disc/candidate_chans/
mkdir -p data/pol_chan_disc/candidate_chan_videos/
mkdir -p data/pol_chan_disc/candidate_chan_videos_top10/
mkdir -p data/pol_chan_disc/candidate_chan_videos_top10_comments_work/
mkdir -p data/pol_chan_disc/candidate_chan_videos_top10_comments/
mkdir -p data/pol_chan_disc/candidate_chan_videos_top10_cat_info/
mkdir -p data/pol_chan_disc/all_commenter_subs/
mkdir -p data/pol_chan_disc/commenter_need_sub_scrap/
mkdir -p data/pol_chan_disc/all_commenter_subs_work/
mkdir -p data/pol_chan_disc/all_commenter_subs_selenium/
mkdir -p data/pol_chan_disc/all_commenter_subs_selenium_work/
mkdir -p data/pol_chan_disc/chan2vec_training_data/


#####################################
# ROUND 0 - INITIAL DATA COLLECTION #
#####################################

# Scrape videos / comments / commenter subscriptions for initial set of labeled channels
cat data/datasets/anton_non_political_channels.txt data/datasets/recfluence_political_channels_20200727.txt > data/pol_chan_disc/candidate_chans/initial_channels.txt

# Videos
# (run on a small t3a instance)
python3 data_collection/python/scrape_channel_latest_videos.py \
	--chan-fp data/pol_chan_disc/candidate_chans/initial_channels.txt \
	--out-fp data/pol_chan_disc/candidate_chan_videos/initial_channels.20200827.videos.txt \
	--scrape-date 2020-08-27

# Comments
# Get top 10 most viewed videos from each channel
python3 data_collection/python/get_top_videos.py \
	--vid-fp data/pol_chan_disc/candidate_chan_videos/initial_channels.20200827.videos.txt \
	--out-fp data/pol_chan_disc/candidate_chan_videos_top10/initial_channels.20200827.top10_videos.vid_ids.txt \
	--most-recent-num 30 --num-keep 10
# Scrape comments for 15K videos using 5 instances (took 15 hours)
# (will start jobs on multiple t3a medium EC2 instances, need to add own --ec2-ip-fp)
python3 data_collection/python/scrape_batch_start.py --scrape-type comment_scrape \
	--scrape-ids-fp data/pol_chan_disc/candidate_chan_videos_top10/initial_channels.20200827.top10_videos.vid_ids.txt \
	--num-jobs-per-inst 2 --ec2-ip-fp data_collection/configs/comment_scrape_instances.20200827.txt \
	--work-dir data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827 \
	--data-loc-fp data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827/data_loc.txt \
	--aws-access-id YOUR_ID --aws-secret-key YOUR_SECRET_KEY
# OPTIONAL - Monitor scrape
python3 data_collection/python/monitor_scrape.py --data-loc-fp data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827/data_loc.txt \
	--col-check 2 --email YOUR_EMAIL_ADDRESS --sub-string comments
# Copy (only after receiving emails that the scrape jobs have finished)
python3 data_collection/python/copy_ec2_data.py \
	--data-loc-fp data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827/data_loc.txt \
	--local-work-dir data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827 \
	--sub-string comments --comb-fp data/pol_chan_disc/candidate_chan_videos_top10_comments/initial_channels_20200827_top10_video_comments.txt
python3 data_collection/python/copy_ec2_data.py \
	--data-loc-fp data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827/data_loc.txt \
	--local-work-dir data/pol_chan_disc/candidate_chan_videos_top10_comments_work/init_chans_20200827 \
	--sub-string cat_info --comb-fp data/pol_chan_disc/candidate_chan_videos_top10_cat_info/initial_channels_20200827_top10_video_cat_info.txt

# Commenter subscriptions
# Identify commenters that haven't had subs scraped yet
python3 data_collection/python/get_new_commenter_ids.py \
	--all-com-subs-dir data/pol_chan_disc/all_commenter_subs \
	--comments-fp data/pol_chan_disc/candidate_chan_videos_top10_comments/initial_channels_20200827_top10_video_comments.txt \
	--all-commenter-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_all_commenters.txt \
	--need-sub-scrape-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_new_commenters.txt
# Scrape commenter subscriptions (4260 per hour per node)
# (will start jobs on multiple t3a small EC2 instances, need to add own --ec2-ip-fp)
python3 data_collection/python/scrape_batch_start.py --scrape-type subs_curl_scrape --ec2-setup-script-fp data_collection/sh/ec2_curl_setup.sh \
	--scrape-ids-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_new_commenters.txt \
	--num-jobs-per-inst 3 --ec2-ip-fp data_collection/configs/commenter_sub_scrape_instances.20200827.txt \
	--work-dir data/pol_chan_disc/all_commenter_subs_work/init_chan_commenter_subs_20200827 \
	--data-loc-fp data/pol_chan_disc/all_commenter_subs_work/init_chan_commenter_subs_20200827/data_loc.txt \
	--aws-access-id YOUR_ID --aws-secret-key YOUR_SECRET_KEY
# OPTIONAL - Monitor scrape
python3 data_collection/python/monitor_scrape.py --data-loc-fp data/pol_chan_disc/all_commenter_subs_work/init_chan_commenter_subs_20200827/data_loc.txt \
	--col-check 1 --email YOUR_EMAIL_ADDRESS
# Copy (only after receiving emails that the scrape jobs have finished)
python3 data_collection/python/copy_ec2_data.py \
	--data-loc-fp data/pol_chan_disc/all_commenter_subs_work/init_chan_commenter_subs_20200827/data_loc.txt \
	--local-work-dir data/pol_chan_disc/all_commenter_subs_work/init_chan_commenter_subs_20200827 \
	--comb-fp data/pol_chan_disc/all_commenter_subs/initial_channels_commenter_subs_20200827.txt

# Commenter subscriptions - selenium
# (7.5K per day per medium instance?)
# If end up doing sample of 10 subscribers for 20,000 channels, will be 200,000 subscribers at 7.5K per day ~ 27 medium instance days (probably under $50)
# This will result in 30M more subs (many that are more likely tail oriented) (guessing 30M for curl sub scrapes)
# Sample 10 commenters with 25+ subs from the last set of candidate channels
python3 data_collection/python/sample_chan_commenters.py --num-samp-commenters 10 --min-subs 25 \
	--comments-fp data/pol_chan_disc/candidate_chan_videos_top10_comments/initial_channels_20200827_top10_video_comments.txt \
	--all-com-subs-dir data/pol_chan_disc/all_commenter_subs \
	--all-com-sel-subs-dir data/pol_chan_disc/all_commenter_subs_selenium \
	--commenters-samp-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_samp_commenters.txt \
	--commenters-samp-need-sel-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_samp_commenters_need_sel_scrape.txt
# Run selenium scrape
python3 data_collection/python/scrape_batch_start.py --scrape-type subs_selenium_scrape \
	--scrape-ids-fp data/pol_chan_disc/commenter_need_sub_scrap/initial_channels_20200827_top10_video_samp_commenters_need_sel_scrape.txt \
	--num-jobs-per-inst 2 --ec2-ip-fp data_collection/configs/commenter_sub_selenium_scrape_instances.20200827.txt \
	--work-dir data/pol_chan_disc/all_commenter_subs_selenium_work/init_chan_commenter_subs_selenium_20200827 \
	--data-loc-fp data/pol_chan_disc/all_commenter_subs_selenium_work/init_chan_commenter_subs_selenium_20200827/data_loc.txt \
	--aws-access-id YOUR_ID --aws-secret-key YOUR_SECRET_KEY
# OPTIONAL - Monitor scrape
python3 data_collection/python/monitor_scrape.py --data-loc-fp data/pol_chan_disc/all_commenter_subs_selenium_work/init_chan_commenter_subs_selenium_20200827/data_loc.txt \
	--col-check 1 --email YOUR_EMAIL_ADDRESS
# Copy (only after receiving emails that the scrape jobs have finished)
python3 data_collection/python/copy_ec2_data.py \
	--data-loc-fp data/pol_chan_disc/all_commenter_subs_selenium_work/init_chan_commenter_subs_selenium_20200827/data_loc.txt \
	--local-work-dir data/pol_chan_disc/all_commenter_subs_selenium_work/init_chan_commenter_subs_selenium_20200827 \
	--comb-fp data/pol_chan_disc/all_commenter_subs_selenium/initial_channels_commenter_subs_selenium_20200827.txt


####################################
# ROUND 0 - IDENTIFY NEW POL CHANS #
####################################

# Generate data for word2vec
# All channels with 5+ scrap subs (no reason to change this now)
python3 chan2vec/python/generate_training_data.py \
	--commenter-config-fp data/pol_chan_disc/chan2vec_training_data_configs/initial_channels_td_config_20200827.json \
	--chan-docs-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.docs.txt \
	--chan-info-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.txt
shuf data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.docs.txt > data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.docs.shuf.txt
"""
Scrape stats-
Eligible commenters: 935356
Commenters no subs: 637427
Commenters w/ subs: 289205
Eligible sel commenters: 14146
Commenters selenium no subs: 204
Commenters selenium w/ subs: 13943
All channels found: 1767500
Channels w/ min scrap subs: 197623
Total combined subs: 10,472,297
"""

# Generate embeddings
# NEED TO INSTALL WORD2VEC: https://code.google.com/archive/p/word2vec/
cd ~/word2vec/
cp ~/chan2vec/data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.docs.shuf.txt temp.in
time ./word2vec -train temp.in -output temp.out -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 0 -iter 15
cp temp.out ~/chan2vec/data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.vectors.txt

# Create new labeled DS (with and without heuristic negative)
python3 experiments/python/generate_pol_class_labs.py \
	--pos-fp data/datasets/recfluence_political_channels_20200727.txt \
	--neg-fp data/datasets/anton_non_political_channels.txt \
	--filter-fp data/datasets/pol_class_ds_filter.txt \
	--min-heuristic-neg-subs 3000000 \
	--vec-chan-info-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.txt \
	--out-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_lab.txt \
	--out-no-heur-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_no_heur_lab.txt

# Generate preds for all vectors (leave one out cross validation..)
cut -f 1 data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.txt \
    > data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.chan_ids.txt
python3 chan2vec/python/chan2vec_knn.py \
        --vec-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.vectors.txt \
        --chan-info-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.txt \
        --label-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_no_heur_lab.txt \
        --score-chan-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.chan_ids.txt \
        --out-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_preds.txt \
        --num-neighbs 10 --use-gpu True --bin-prob True

# Pull metrics on performance (only for Recfluence and Anton channels) and coverage (chans and total subs)
# Largest threshold where recall is greater >= 0.9
python3 chan2vec/python/gen_pred_stats_bin.py \
        --lab-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_no_heur_lab.txt \
        --score-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_preds.txt \
        --no-fold-lab-col True --pred-thresh 0.9
"""
Num instances: 1488
AUC:           0.9902
Accuracy:      0.9603
Precision:     0.9715
Recall:        0.9497
"""

# Get new candidate channels (those with 3M+ subs or predicted to be political w/ 10K+ subs)
python3 experiments/python/get_new_candidate_chans.py \
	--chan-info-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.chan_info.txt \
	--scores-fp data/pol_chan_disc/chan2vec_training_data/chan2vec_initial_channels_ds.pol_class_preds.txt \
	--prev-candidate-fps data/pol_chan_disc/candidate_chans/initial_channels.txt \
	--out-fp data/pol_chan_disc/candidate_chans/round1_channels.txt
"""
All previous candidate negative predictions: 750
All previous candidate positive predictions: 740
All previous candidate positive predictions - Tot Subs: 425,651,806
New heuristic chans: 3,643
New positive chans: 12,510
New positive chans - Tot Subs: 1,699,934,476
"""
